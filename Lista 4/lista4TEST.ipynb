{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alvaro\\anaconda3\\envs\\pln\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    uk manufacturing sector continue face serious ...\n",
       "1    climate change fight aids leading list concern...\n",
       "2    shares europe leading reinsurers travel firms ...\n",
       "3    shares india largest power producer national t...\n",
       "4    luxury goods group lvmh sold loss making chris...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../clean_dataset_no_stemming.csv\")\n",
    "display(df['clean_text'].head(5))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    511\n",
      "0    510\n",
      "2    417\n",
      "3    401\n",
      "4    386\n",
      "Name: binary_target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "new_target = {'sport': 1, 'business': 0, 'politics': 2, 'tech': 3, 'entertainment': 4}\n",
    "df['binary_target'] = df['target'].map(new_target)\n",
    "print(df['binary_target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>binary_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289.txt</td>\n",
       "      <td>UK economy facing 'major risks'\\n</td>\n",
       "      <td>The UK manufacturing sector will continue to f...</td>\n",
       "      <td>business</td>\n",
       "      <td>uk manufacturing sector continue face serious ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504.txt</td>\n",
       "      <td>Aids and climate top Davos agenda\\n</td>\n",
       "      <td>Climate change and the fight against Aids are ...</td>\n",
       "      <td>business</td>\n",
       "      <td>climate change fight aids leading list concern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262.txt</td>\n",
       "      <td>Asian quake hits European shares\\n</td>\n",
       "      <td>Shares in Europe's leading reinsurers and trav...</td>\n",
       "      <td>business</td>\n",
       "      <td>shares europe leading reinsurers travel firms ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276.txt</td>\n",
       "      <td>India power shares jump on debut\\n</td>\n",
       "      <td>Shares in India's largest power producer, Nati...</td>\n",
       "      <td>business</td>\n",
       "      <td>shares india largest power producer national t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510.txt</td>\n",
       "      <td>Lacroix label bought by US firm\\n</td>\n",
       "      <td>Luxury goods group LVMH has sold its loss-maki...</td>\n",
       "      <td>business</td>\n",
       "      <td>luxury goods group lvmh sold loss making chris...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>253.txt</td>\n",
       "      <td>Old Firm pair handed suspensions\\n</td>\n",
       "      <td>Celtic's Henri Camara and Nacho Novo of Ranger...</td>\n",
       "      <td>sport</td>\n",
       "      <td>celtic henri camara nacho novo rangers suspend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>247.txt</td>\n",
       "      <td>Real will finish abandoned match\\n</td>\n",
       "      <td>Real Madrid and Real Socieded will play the fi...</td>\n",
       "      <td>sport</td>\n",
       "      <td>real madrid real socieded play final six minut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>509.txt</td>\n",
       "      <td>Melzer shocks Agassi in San Jose\\n</td>\n",
       "      <td>Second seed Andre Agassi suffered a comprehens...</td>\n",
       "      <td>sport</td>\n",
       "      <td>second seed andre agassi suffered comprehensiv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>290.txt</td>\n",
       "      <td>O'Gara revels in Ireland victory\\n</td>\n",
       "      <td>Ireland fly-half Ronan O'Gara hailed his side'...</td>\n",
       "      <td>sport</td>\n",
       "      <td>ireland fly half ronan ogara hailed side victo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>284.txt</td>\n",
       "      <td>Hodgson shoulders England blame\\n</td>\n",
       "      <td>Fly-half Charlie Hodgson admitted his wayward ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>fly half charlie hodgson admitted wayward kick...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename                                title  \\\n",
       "0     289.txt    UK economy facing 'major risks'\\n   \n",
       "1     504.txt  Aids and climate top Davos agenda\\n   \n",
       "2     262.txt   Asian quake hits European shares\\n   \n",
       "3     276.txt   India power shares jump on debut\\n   \n",
       "4     510.txt    Lacroix label bought by US firm\\n   \n",
       "...       ...                                  ...   \n",
       "1819  253.txt   Old Firm pair handed suspensions\\n   \n",
       "1820  247.txt   Real will finish abandoned match\\n   \n",
       "1821  509.txt   Melzer shocks Agassi in San Jose\\n   \n",
       "1822  290.txt   O'Gara revels in Ireland victory\\n   \n",
       "1823  284.txt    Hodgson shoulders England blame\\n   \n",
       "\n",
       "                                                article    target  \\\n",
       "0     The UK manufacturing sector will continue to f...  business   \n",
       "1     Climate change and the fight against Aids are ...  business   \n",
       "2     Shares in Europe's leading reinsurers and trav...  business   \n",
       "3     Shares in India's largest power producer, Nati...  business   \n",
       "4     Luxury goods group LVMH has sold its loss-maki...  business   \n",
       "...                                                 ...       ...   \n",
       "1819  Celtic's Henri Camara and Nacho Novo of Ranger...     sport   \n",
       "1820  Real Madrid and Real Socieded will play the fi...     sport   \n",
       "1821  Second seed Andre Agassi suffered a comprehens...     sport   \n",
       "1822  Ireland fly-half Ronan O'Gara hailed his side'...     sport   \n",
       "1823  Fly-half Charlie Hodgson admitted his wayward ...     sport   \n",
       "\n",
       "                                             clean_text  binary_target  \n",
       "0     uk manufacturing sector continue face serious ...              0  \n",
       "1     climate change fight aids leading list concern...              0  \n",
       "2     shares europe leading reinsurers travel firms ...              0  \n",
       "3     shares india largest power producer national t...              0  \n",
       "4     luxury goods group lvmh sold loss making chris...              0  \n",
       "...                                                 ...            ...  \n",
       "1819  celtic henri camara nacho novo rangers suspend...              1  \n",
       "1820  real madrid real socieded play final six minut...              1  \n",
       "1821  second seed andre agassi suffered comprehensiv...              1  \n",
       "1822  ireland fly half ronan ogara hailed side victo...              1  \n",
       "1823  fly half charlie hodgson admitted wayward kick...              1  \n",
       "\n",
       "[1021 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_balanceado = df[(df['binary_target']==0) | (df['binary_target']==1)]\n",
    "display(dataset_balanceado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset_balanceado['clean_text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "counter = 0\n",
    "max_possible_len = 512\n",
    "for index,i in enumerate(tokenized.values):\n",
    "    # cropping input to length 512\n",
    "    if len(i) > max_possible_len:\n",
    "        tokenized.values[index] = i[:max_possible_len-1]\n",
    "        # get the length\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(tokenized.values[index])\n",
    "print(max_len)\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021, 511)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021, 511)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pln')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf1c6814613082b7ed494c008cea4b2304f61caa5fbdd7ed02b9c65d1140fe1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
